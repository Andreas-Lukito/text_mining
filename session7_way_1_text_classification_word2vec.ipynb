{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K5upzzBTXmaw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BDI0c-QCXmay"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67RNMYEzXmaz"
   },
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sOCUYYxAXma0"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n5DiM9-_Xma1",
    "outputId": "84207e54-6ab6-4b86-cc51-f3c686292e5e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are Thursday's biggest analyst calls: App...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy Las Vegas Sands as travel to Singapore bui...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piper Sandler downgrades DocuSign to sell, cit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analysts react to Tesla's latest earnings, bre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix and its peers are set for a ‘return to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Here are Thursday's biggest analyst calls: App...      0\n",
       "1  Buy Las Vegas Sands as travel to Singapore bui...      0\n",
       "2  Piper Sandler downgrades DocuSign to sell, cit...      0\n",
       "3  Analysts react to Tesla's latest earnings, bre...      0\n",
       "4  Netflix and its peers are set for a ‘return to...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S666OdamXma1",
    "outputId": "fb814094-9d2d-4f1a-a6eb-bdf45c01ba91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16990, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "f42eKORlXma2",
    "outputId": "77472c85-4450-41ac-8d40-5e265f20cee5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2     3545\n",
       "18    2118\n",
       "14    1822\n",
       "9     1557\n",
       "5      987\n",
       "16     985\n",
       "1      837\n",
       "19     823\n",
       "7      624\n",
       "6      524\n",
       "15     501\n",
       "17     495\n",
       "12     487\n",
       "13     471\n",
       "4      359\n",
       "3      321\n",
       "0      255\n",
       "8      166\n",
       "10      69\n",
       "11      44\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QY0SOWrTXma2"
   },
   "source": [
    "**Data Cleansing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WgCEv2wzXma2"
   },
   "outputs": [],
   "source": [
    "def cleansing(df):\n",
    "    df_clean=df.str.lower()\n",
    "    df_clean=[re.sub(r\"\\d+\",\"\",i )for i in df_clean]\n",
    "    df_clean=[re.sub(r'[^\\w]', ' ', i)for i in df_clean]\n",
    "    df_clean=[re.sub(r'\\s+',' ',i)for i in df_clean]\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4qjK7OWdXma2"
   },
   "outputs": [],
   "source": [
    "df['clean_text']=cleansing(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uPPHlgW5Xma2",
    "outputId": "3ce46b63-32b2-4f62-fb4a-4658d0887a9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy las vegas sands as travel to singapore builds wells fargo says https t co flswicz'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XVRW9_hhXma3",
    "outputId": "d899afd7-1c86-4265-beb1-e4b2dd9a5bd7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are Thursday's biggest analyst calls: App...</td>\n",
       "      <td>0</td>\n",
       "      <td>here are thursday s biggest analyst calls appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy Las Vegas Sands as travel to Singapore bui...</td>\n",
       "      <td>0</td>\n",
       "      <td>buy las vegas sands as travel to singapore bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piper Sandler downgrades DocuSign to sell, cit...</td>\n",
       "      <td>0</td>\n",
       "      <td>piper sandler downgrades docusign to sell citi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analysts react to Tesla's latest earnings, bre...</td>\n",
       "      <td>0</td>\n",
       "      <td>analysts react to tesla s latest earnings brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix and its peers are set for a ‘return to...</td>\n",
       "      <td>0</td>\n",
       "      <td>netflix and its peers are set for a return to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Here are Thursday's biggest analyst calls: App...      0   \n",
       "1  Buy Las Vegas Sands as travel to Singapore bui...      0   \n",
       "2  Piper Sandler downgrades DocuSign to sell, cit...      0   \n",
       "3  Analysts react to Tesla's latest earnings, bre...      0   \n",
       "4  Netflix and its peers are set for a ‘return to...      0   \n",
       "\n",
       "                                          clean_text  \n",
       "0  here are thursday s biggest analyst calls appl...  \n",
       "1  buy las vegas sands as travel to singapore bui...  \n",
       "2  piper sandler downgrades docusign to sell citi...  \n",
       "3  analysts react to tesla s latest earnings brea...  \n",
       "4  netflix and its peers are set for a return to ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bQDD_aEMXma3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, log_loss\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsMLm2XjXma3"
   },
   "source": [
    "**Split train test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "owEYuirDXma3"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['clean_text'], df['label'], test_size = 0.2, random_state = 42,stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EZwWa8dkXma3",
    "outputId": "58c091ea-4919-4cd7-b686-0eca768fe259",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5922     it s ecb rate decision day here s what to expe...\n",
       "13498    twitter users were quick to spot liz truss see...\n",
       "4517     jetblue announces webcast of second quarter ea...\n",
       "16161     calm cal maine foods stock ticks higher on re...\n",
       "1745     tower semiconductor and cadence expand collabo...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9zBn3UHXma4"
   },
   "source": [
    "**Tokenize the words in the train and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XRyCLNBWXma4",
    "tags": []
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/andrew/nltk_data'\n    - '/home/andrew/miniconda3/envs/i_guess_thats_all-env/nltk_data'\n    - '/home/andrew/miniconda3/envs/i_guess_thats_all-env/share/nltk_data'\n    - '/home/andrew/miniconda3/envs/i_guess_thats_all-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m word_token_train = \u001b[43m[\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      2\u001b[39m word_token_test = [word_tokenize(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x_test]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m word_token_train = [\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x_train]\n\u001b[32m      2\u001b[39m word_token_test = [word_tokenize(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x_test]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/i_guess_thats_all-env/lib/python3.11/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/i_guess_thats_all-env/lib/python3.11/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/i_guess_thats_all-env/lib/python3.11/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/i_guess_thats_all-env/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/i_guess_thats_all-env/lib/python3.11/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/i_guess_thats_all-env/lib/python3.11/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/andrew/nltk_data'\n    - '/home/andrew/miniconda3/envs/i_guess_thats_all-env/nltk_data'\n    - '/home/andrew/miniconda3/envs/i_guess_thats_all-env/share/nltk_data'\n    - '/home/andrew/miniconda3/envs/i_guess_thats_all-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "word_token_train = [word_tokenize(i) for i in x_train]\n",
    "word_token_test = [word_tokenize(i) for i in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvC1PisqXma4",
    "outputId": "0c5a2e21-e2a4-401f-b10e-ec5af3ee411d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it',\n",
       "  's',\n",
       "  'ecb',\n",
       "  'rate',\n",
       "  'decision',\n",
       "  'day',\n",
       "  'here',\n",
       "  's',\n",
       "  'what',\n",
       "  'to',\n",
       "  'expect',\n",
       "  'via',\n",
       "  'weberalexander',\n",
       "  'amp',\n",
       "  'carolynnlook',\n",
       "  'https',\n",
       "  't',\n",
       "  'co',\n",
       "  'isqgdue']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token_train[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXnse5j8Xma4"
   },
   "source": [
    "**Remove Stop word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-HvRurBXma4",
    "outputId": "630252a8-0008-4c42-879d-f2f5d4be14f0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liliayu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['can',\n",
       " 'as',\n",
       " 'over',\n",
       " 'will',\n",
       " 'being',\n",
       " 'your',\n",
       " 'those',\n",
       " 'himself',\n",
       " 'or',\n",
       " 'when',\n",
       " 'nor',\n",
       " \"hadn't\",\n",
       " 'theirs',\n",
       " 'against',\n",
       " 'my',\n",
       " 'in',\n",
       " 'haven',\n",
       " 'from',\n",
       " 'themselves',\n",
       " 'while']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# Get a list of stop words in the Indonesian language\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Display the top 20 stop words\n",
    "list(stop_words)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ExZEhXiXma4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove stopwords from each sublist in word_token_train and test\n",
    "filtered_tokens_train = [[word for word in sublist if word not in stop_words] for sublist in word_token_train]\n",
    "filtered_tokens_test = [[word for word in sublist if word not in stop_words] for sublist in word_token_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaipVXGPXma5",
    "outputId": "62488f41-863c-4a5e-8cf1-fc1caf872dd9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ecb',\n",
       "  'rate',\n",
       "  'decision',\n",
       "  'day',\n",
       "  'expect',\n",
       "  'via',\n",
       "  'weberalexander',\n",
       "  'amp',\n",
       "  'carolynnlook',\n",
       "  'https',\n",
       "  'co',\n",
       "  'isqgdue'],\n",
       " ['twitter',\n",
       "  'users',\n",
       "  'quick',\n",
       "  'spot',\n",
       "  'liz',\n",
       "  'truss',\n",
       "  'seemingly',\n",
       "  'recreating',\n",
       "  'outfit',\n",
       "  'margaret',\n",
       "  'thatcher',\n",
       "  'appearance',\n",
       "  'channel',\n",
       "  'tory',\n",
       "  'leadership',\n",
       "  'debate',\n",
       "  'https',\n",
       "  'co',\n",
       "  'vsiioegrz']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens_train[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84AAAHTgXma5"
   },
   "source": [
    "**Text representation and Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6uhhBtDXma5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Skipgram\n",
    "model_skipgram = gensim.models.Word2Vec(filtered_tokens_train, min_count = 3, vector_size = 50, window = 5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfeDsU7qXma5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get word vectors\n",
    "def get_word_vector(model, word):\n",
    "    try:\n",
    "        return model.wv.get_vector(word)\n",
    "    except KeyError:\n",
    "        # If the word is not in the model, return None\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2J3SI_NXma5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate word vectors for filtered tokens using the specified word embedding model\n",
    "def generate_word_vectors(filtered_tokens, word_vector_model, vector_length=50):\n",
    "    # Initialize an empty list to store the resulting word vectors\n",
    "    X_vectors = []\n",
    "\n",
    "    # Iterate through each list of filtered tokens in the dataset\n",
    "    for tokens in filtered_tokens:\n",
    "        # Use the get_word_vector function to retrieve word vectors for each token\n",
    "        vectorized_tokens = [get_word_vector(word_vector_model, word) for word in tokens]\n",
    "\n",
    "         # Remove None values (words not present in the model) from the list of vectors\n",
    "        vectorized_tokens = [vector for vector in vectorized_tokens if vector is not None]\n",
    "\n",
    "        # If there are valid vectors, compute the average vector\n",
    "        if vectorized_tokens:\n",
    "            average_vector = sum(vectorized_tokens) / len(vectorized_tokens)\n",
    "            X_vectors.append(average_vector)\n",
    "        else:\n",
    "            # If no valid vectors are present, use a zero vector as a placeholder\n",
    "            X_vectors.append([0] * vector_length)\n",
    "\n",
    "    # Return the list of resulting word vectors\n",
    "    return X_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qN0-wzAFXma6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transforming tokens in x_test into vectors using Skipgram model\n",
    "x_train_vectors_skipgram = generate_word_vectors(filtered_tokens_train, model_skipgram)\n",
    "x_test_vectors_skipgram = generate_word_vectors(filtered_tokens_test, model_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAaY5QqNXma6",
    "outputId": "34746652-2d74-44c6-ae0b-f88bf6048b43",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.21741273, -0.00356028,  0.09416561,  0.10089619,  0.0910439 ,\n",
       "        -0.16837142,  0.60920864,  0.35092878, -0.21787873,  0.10309077,\n",
       "         0.18494155, -0.36086994, -0.04488184, -0.05125307, -0.21484652,\n",
       "        -0.22467075,  0.25970855,  0.00931355, -0.24847074, -0.10399978,\n",
       "         0.25260985,  0.49241284,  0.35422885, -0.22224122,  0.22562155,\n",
       "        -0.01508421,  0.15638095, -0.12423069, -0.21262318, -0.12599792,\n",
       "         0.0417222 ,  0.38448933, -0.24292058,  0.21185866, -0.38258132,\n",
       "         0.4510027 ,  0.15048984, -0.05780258, -0.16138749, -0.0950003 ,\n",
       "         0.5000799 ,  0.07071552, -0.08694933,  0.05660988,  0.44913128,\n",
       "        -0.01768802, -0.27912882, -0.39243996,  0.27753785,  0.42333868],\n",
       "       dtype=float32),\n",
       " array([-0.09671163,  0.1697121 ,  0.07092498, -0.10292488, -0.05555817,\n",
       "        -0.20847143,  0.4213042 ,  0.3746483 , -0.15316257, -0.02408643,\n",
       "         0.2845458 , -0.2828379 , -0.26983035, -0.11992088, -0.1079844 ,\n",
       "        -0.21045396,  0.02946852, -0.31724638, -0.14648749, -0.12656882,\n",
       "         0.15459312,  0.24736014,  0.4129958 , -0.30138627,  0.08219949,\n",
       "        -0.05585789,  0.03551892, -0.19398329, -0.0707155 ,  0.17234518,\n",
       "         0.18731837,  0.23628461, -0.4573412 ,  0.16907907, -0.09337187,\n",
       "         0.3658497 ,  0.26022342,  0.17480314, -0.19938773, -0.3094673 ,\n",
       "         0.42365125, -0.21019357,  0.01083392,  0.10900348,  0.37377268,\n",
       "         0.01675201, -0.1837228 , -0.32443166,  0.10337979,  0.14983065],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_vectors_skipgram[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd1YfCFrYsMY"
   },
   "source": [
    "**SVM for text classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-BsM__TXma6",
    "outputId": "ae6f50a2-6ce8-4815-fe63-b9d64471d7c7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(random_state=42)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_class = svm.LinearSVC( random_state=42)\n",
    "svm_class.fit(x_train_vectors_skipgram, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7iMXvdNXma6"
   },
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNpkA-xGXma6"
   },
   "outputs": [],
   "source": [
    "test_svm_class=svm_class.predict(x_test_vectors_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsJPDN5wXma6",
    "outputId": "3dc42abf-64c0-409b-83ef-b6b70897a300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.10      0.16        51\n",
      "           1       0.69      0.69      0.69       167\n",
      "           2       0.57      0.79      0.66       709\n",
      "           3       0.00      0.00      0.00        64\n",
      "           4       0.97      0.89      0.93        72\n",
      "           5       0.84      0.95      0.89       198\n",
      "           6       0.68      0.74      0.71       105\n",
      "           7       0.81      0.66      0.73       125\n",
      "           8       0.71      0.30      0.43        33\n",
      "           9       0.48      0.31      0.38       311\n",
      "          10       0.00      0.00      0.00        14\n",
      "          11       0.00      0.00      0.00         9\n",
      "          12       0.69      0.49      0.57        97\n",
      "          13       1.00      0.01      0.02        94\n",
      "          14       0.56      0.71      0.63       364\n",
      "          15       0.68      0.41      0.51       100\n",
      "          16       0.70      0.80      0.75       197\n",
      "          17       0.85      0.69      0.76        99\n",
      "          18       0.67      0.85      0.75       424\n",
      "          19       0.48      0.25      0.33       165\n",
      "\n",
      "    accuracy                           0.64      3398\n",
      "   macro avg       0.59      0.48      0.50      3398\n",
      "weighted avg       0.63      0.64      0.61      3398\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liliayu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/liliayu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/liliayu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, test_svm_class, target_names=['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6NGmyrUXma7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "i_guess_thats_all-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
